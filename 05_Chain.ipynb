{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c7a78c",
   "metadata": {},
   "source": [
    "## Normal Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Kenya is **Nairobi**.\n"
     ]
    }
   ],
   "source": [
    "from json import load\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.6,\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "template = PromptTemplate.from_template(\"You are a helpful assistant. Answer the question: {question}\")\n",
    "\n",
    "chain = template | llm | parser\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is the capital of Kenya?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5fa2f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      +-------------+      \n",
      "      | PromptInput |      \n",
      "      +-------------+      \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7d682",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dffee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 key points from the summary about Generative AI:\n",
      "\n",
      "1.  **Creation of New Content:** Generative AI's core function is to produce entirely new and original outputs.\n",
      "2.  **Learning from Existing Data:** These AI systems learn patterns and structures by analyzing large datasets of existing content.\n",
      "3.  **Mimicking or Inspired by Data:** The generated content is designed to resemble or be influenced by the data it was trained on.\n",
      "4.  **Beyond Analysis:** Unlike analytical AI, generative AI goes beyond simply understanding information to actively creating it.\n",
      "5.  **Diverse Applications:** It can generate various forms of content, including text, images, music, and code.\n"
     ]
    }
   ],
   "source": [
    "from json import load\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.6,\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "seq_template1 = PromptTemplate.from_template(\"\"\"\n",
    "Give me a short summary on topic : {topic}\n",
    "\"\"\")\n",
    "\n",
    "seq_template2 = PromptTemplate.from_template(\"\"\"\n",
    "Based on the summary, give me 5 key points on it \\n {summary}\n",
    "\"\"\")\n",
    "\n",
    "seq_chain = seq_template1 | llm | parser | seq_template2 | llm | parser\n",
    "\n",
    "seq_result = seq_chain.invoke({\"topic\": \"Generative AI\"})\n",
    "\n",
    "print(seq_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12886265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      +-------------+      \n",
      "      | PromptInput |      \n",
      "      +-------------+      \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "seq_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d7f80",
   "metadata": {},
   "source": [
    "## Parallel Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018a9494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a comprehensive output combining the provided information:\n",
      "\n",
      "**Comprehensive Output:**\n",
      "\n",
      "Generative AI is a transformative type of artificial intelligence distinguished by its ability to **create new content**. This content can take various forms, including **text, images, music, and code**, all generated by the AI based on the intricate patterns and vast amounts of information it has assimilated from extensive datasets.\n",
      "\n",
      "Crucially, generative AI differs from traditional AI approaches that primarily focus on analyzing or classifying existing data. Instead, generative AI is designed to **produce original outputs**. These outputs often exhibit a remarkable degree of human-likeness and can be entirely novel, pushing the boundaries of what AI can achieve.\n",
      "\n",
      "**Key Points to Analyze (based on the provided summary):**\n",
      "\n",
      "1.  **Content Creation:** Generative AI's core capability is the creation of new content.\n",
      "2.  **Diverse Output Forms:** The content generated can span multiple modalities such as text, images, music, and code.\n",
      "3.  **Learning from Datasets:** Generative AI learns patterns and information from massive datasets to inform its creations.\n",
      "4.  **Distinction from Traditional AI:** It differentiates itself from traditional AI by producing original outputs, rather than just analyzing or classifying existing data.\n",
      "5.  **Human-like and Novel Outputs:** The generated content can be remarkably human-like and novel.\n"
     ]
    }
   ],
   "source": [
    "from json import load\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.6,\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "par_template1 = PromptTemplate.from_template(\"\"\"\n",
    "Give me a short summary on topic : {topic}\n",
    "\"\"\")\n",
    "\n",
    "par_template2 = PromptTemplate.from_template(\"\"\"\n",
    "Based on the summary, give me 5 key points on it \\n {topic}\n",
    "\"\"\")\n",
    "\n",
    "par_chain1 = par_template1 | llm | parser\n",
    "par_chain2 = par_template2 | llm | parser\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"summary\": par_chain1,\n",
    "        \"key_points\": par_chain2\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "mer_template = PromptTemplate.from_template(\"\"\"\n",
    "Combine the following information into a comprehensive output:\n",
    "\n",
    "Summary: {summary}\n",
    "Key Points: {key_points}\n",
    "\"\"\")\n",
    "\n",
    "mer_chain = mer_template | llm | parser\n",
    "\n",
    "final_chain = parallel_chain | mer_chain\n",
    "\n",
    "par_result = final_chain.invoke({\"topic\": \"Generative AI\"})\n",
    "\n",
    "print(par_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f3199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                +-----------------------------------+                  \n",
      "                | Parallel<summary,key_points>Input |                  \n",
      "                +-----------------------------------+                  \n",
      "                       ***                   ***                       \n",
      "                   ****                         ****                   \n",
      "                 **                                 **                 \n",
      "    +----------------+                          +----------------+     \n",
      "    | PromptTemplate |                          | PromptTemplate |     \n",
      "    +----------------+                          +----------------+     \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "+------------------------+                  +------------------------+ \n",
      "| ChatGoogleGenerativeAI |                  | ChatGoogleGenerativeAI | \n",
      "+------------------------+                  +------------------------+ \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "    +-----------------+                         +-----------------+    \n",
      "    | StrOutputParser |                         | StrOutputParser |    \n",
      "    +-----------------+                         +-----------------+    \n",
      "                       ***                   ***                       \n",
      "                          ****           ****                          \n",
      "                              **       **                              \n",
      "                +------------------------------------+                 \n",
      "                | Parallel<summary,key_points>Output |                 \n",
      "                +------------------------------------+                 \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                          +----------------+                           \n",
      "                          | PromptTemplate |                           \n",
      "                          +----------------+                           \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                      +------------------------+                       \n",
      "                      | ChatGoogleGenerativeAI |                       \n",
      "                      +------------------------+                       \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                          +-----------------+                          \n",
      "                          | StrOutputParser |                          \n",
      "                          +-----------------+                          \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                      +-----------------------+                        \n",
      "                      | StrOutputParserOutput |                        \n",
      "                      +-----------------------+                        \n"
     ]
    }
   ],
   "source": [
    "final_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553aa72b",
   "metadata": {},
   "source": [
    "## Conditional Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5caea6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m str_parser = StrOutputParser()\n\u001b[32m     19\u001b[39m pydantic_parser = PydanticOutputParser(pydantic_object=ReviewSentiment)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m con_template1 = \u001b[43mPromptTemplate\u001b[49m.from_template(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[33mGive me sentiment of the movie review \u001b[39m\u001b[38;5;132;01m{review}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33mformat_instruction: \u001b[39m\u001b[38;5;132;01m{format_instruction}\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33m\"\"\"\u001b[39m,\n\u001b[32m     25\u001b[39m partial_variables={\u001b[33m\"\u001b[39m\u001b[33mformat_instruction\u001b[39m\u001b[33m\"\u001b[39m: pydantic_parser.get_format_instructions()}\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m review_chain = con_template1 | llm | pydantic_parser\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#print(review_chain.invoke({\"review\": \"This movie was fantastic! The acting was superb and the plot was engaging.\"}))\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableBranch\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  # free + fast\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "class ReviewSentiment(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = Field(description=\"The sentiment of the review\")\n",
    "\n",
    "str_parser = StrOutputParser()\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=ReviewSentiment)\n",
    "\n",
    "con_template1 = PromptTemplate.from_template(\"\"\"\n",
    "Give me sentiment of the movie review {review} \\n\n",
    "format_instruction: {format_instruction}\n",
    "\"\"\",\n",
    "partial_variables={\"format_instruction\": pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "review_chain = con_template1 | llm | pydantic_parser\n",
    "\n",
    "#print(review_chain.invoke({\"review\": \"This movie was fantastic! The acting was superb and the plot was engaging.\"}))\n",
    "\n",
    "pos_template = PromptTemplate.from_template(\"\"\"\n",
    "The review is positive so write a short appreciation: \\n {review}\n",
    "\"\"\")\n",
    "\n",
    "neg_template = PromptTemplate.from_template(\"\"\"\n",
    "The review is negative so write a short critique: \\n {review}\n",
    "\"\"\")\n",
    "\n",
    "neu_template = PromptTemplate.from_template(\"\"\"\n",
    "The review is neutral so write a short balanced remarks: \\n {review}\n",
    "\"\"\")\n",
    "\n",
    "default_template = PromptTemplate.from_template(\"\"\"\n",
    "I could not detect the clear sentiment. Provide a neutral response: \\n {review}\n",
    "\"\"\")\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: x.sentiment == \"positive\", pos_template | llm | str_parser),\n",
    "    (lambda x: x.sentiment == \"negative\", neg_template | llm | str_parser),\n",
    "    (lambda x: x.sentiment == \"neutral\", neu_template | llm | str_parser),\n",
    "    default_template | llm | str_parser\n",
    ")\n",
    "\n",
    "final_con_chain = review_chain | branch_chain\n",
    "\n",
    "review = \"It was okay - some good moments, but nothing partucularly memorable. Just an average watch.\"\n",
    "\n",
    "print(review_chain.invoke({\"review\": review}))\n",
    "\n",
    "final_con_result = final_con_chain.invoke({\"review\": review})\n",
    "\n",
    "print(final_con_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
